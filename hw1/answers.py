r"""
Use this module to write your answers to the questions in the notebook.

Note: Inside the answer strings you can use Markdown format and also LaTeX
math (delimited with $$).
"""

# ==============
# Part 2 answers

part2_q1 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""
# ==============

# ==============
# Part 3 answers

part3_q1 = r"""
** The delta forces a distances of the samples from the hyperplane W creates. 
The same hyperplane will be reached, but scaled by a constant scalar, which will scale the weights by the same constant**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

part3_q2 = r"""
**We can think of the weights for each class(columns) as a vector representing the "average" 
x example for that class. the class of x is scored by a dot product with x, so the weight will
try to embody a close representation,in the vector space, to x's of that class.

The distance analogy is similar with knn but different in that:
1) different distance metric is used(dot product vs euclidean)**
2) knn is optimized/punished for distance with the top k examples while the linear classifier measures
the average distance from all the dataset


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

part3_q3 = r"""
**
 1) When the learning rate is too high I expect the training loss to not converge at all,
 or be very noisy, i..e jump up and down. Clearly this is not the case as the training loss decreases with time
 When the learning rate is too low, the loss will decrease but at a lower rate, and the final loss would not be 
 as high as possible within the same number of epochs.
 
 It is possible that slightly higher learning rates would lead to lower overall loss but it seems
 like overall the learning rate is good as the loss decreases nicely and approches zero.
 
 2) The model is slightly overfitted. We can see that by the fact that the train accuracy is higher
 then the validation's. That fact remains stable from about epoch 10 
 
 
**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

# ==============

# ==============
# Part 4 answers

part4_q1 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

part4_q2 = r"""
**Your answer:**


Write your answer using **markdown** and $\LaTeX$:
```python
# A code block
a = 2
```
An equation: $e^{i\pi} -1 = 0$

"""

# ==============
